{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from exp.nb_02 import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function: Cross entropy loss\n",
    "\n",
    "Because multi-class label problem \n",
    "\n",
    "Output = preds\n",
    "\n",
    "1. We take e^output for every class or label\n",
    "2. We take the sum(e^outputs)\n",
    "3. Prediction will be e^output/sum(e^outputs) - which adds up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch negative log-liklihood requires log softmax - hence the log\n",
    "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially calculating negative log likelihood only requires the value of the target because the other part of the formula is just a multiplication with 0 - i.e. waste\n",
    "\n",
    "The method below allows us to write negative log likelihood in a very concise manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explanation at Lesson 9 32 mins \n",
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2628, -2.2993, -2.4600, -2.5769, -2.2144, -2.2638, -2.3418, -2.1843,\n",
       "        -2.1761, -2.3145], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we actually only want the value in the 5th position because this is the target - see y_train above\n",
    "sm_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.2638, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2638, -2.3113, -2.2257], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so to calculate the loss of the first 3 values we want the following indices\n",
    "sm_pred[[0,1,2],[5,0,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target): return -input[range(target.shape[0]), target].mean() # range(target.shape[0]) selects all the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nll(sm_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3165, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(nll(log_softmax(pred), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using LogSumExp - when using e^something numbers can get very large - when using floating point numbers they tend to get less accurate as they get very large - two numbers 1000 appart will be treated similarly by computers - which is bad when doing gradient calculations\n",
    "\n",
    "The trick below allows us to do the same calculation without the numerical problems of floating point calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical stability trick\n",
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(logsumexp(pred), pred.logsumexp(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.logsumexp(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(nll(log_softmax(pred), y_train), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch implementaion\n",
    "test_near(F.nll_loss(F.log_softmax(pred, -1), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which is the same as cross_entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(F.cross_entropy(pred, y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic training loop\n",
    "\n",
    "The training loop will repeat over the following steps\n",
    "\n",
    "- get the output of the model on a batch of inputs\n",
    "- compare the output to the labels we have and compute a loss\n",
    "- calculate the gradients of the loss with respect to every parameter of the model\n",
    "- update the parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are grabbing the highest number from the output, with the index being the prediction and check if it's equal to the actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def accuracy(out, yb): return (torch.argmax(out,dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0207, -0.0158, -0.1765, -0.2934,  0.0691,  0.0197, -0.0583,  0.0992,\n",
       "          0.1074, -0.0310], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=64 #batch size\n",
    "\n",
    "xb = x_train[0:bs] # gets a mini-batch from x\n",
    "preds = model(xb) # preds\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3217, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "loss_func(preds,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0938)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds,yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is giving random answers because we have yet to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    # calculate y_hat for linear model\n",
    "    y_hat = x@a\n",
    "    # calculate loss - compare actual with preds\n",
    "    loss = mse(y, y_hat)\n",
    "    # print loss every 10 steps\n",
    "    if t % 10 == 0: print(loss)\n",
    "    # calculate the gradients\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        # subtract learning rate * gradient from parameters\n",
    "        a.sub_(lr* a.grad)\n",
    "        # zero gradients\n",
    "        a.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5 # learning rate\n",
    "epochs = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=784, out_features=50, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=50, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0252,  0.0172, -0.0078,  ...,  0.0220, -0.0137,  0.0261],\n",
       "        [-0.0084, -0.0175, -0.0071,  ...,  0.0012,  0.0263, -0.0118],\n",
       "        [ 0.0219, -0.0201, -0.0089,  ..., -0.0081, -0.0106, -0.0074],\n",
       "        ...,\n",
       "        [ 0.0037, -0.0296, -0.0204,  ..., -0.0264, -0.0237,  0.0078],\n",
       "        [-0.0038,  0.0181,  0.0270,  ..., -0.0292,  0.0166,  0.0039],\n",
       "        [-0.0330, -0.0111,  0.0036,  ...,  0.0162,  0.0171, -0.0315]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent from scratch\n",
    "for epoch in range(epochs):\n",
    "    # calculates number of mini batches // double backslash returns floating point number from integer division \n",
    "    for i in range((n-1)//bs + 1): \n",
    "        start_i = i * bs # 0 64\n",
    "        end_i = start_i + bs # 64 128\n",
    "        loss = loss_func(model(xb), yb) # calculate loss on batch\n",
    "        \n",
    "        loss.backward() # calculate gradients\n",
    "        with torch.no_grad(): # allows for in-place changing of parameters\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_() # sets gradients to zero in place\n",
    "                    l.bias.grad.zero_()                            \n",
    "                    \n",
    "                    # this is \"slow\" because we're going through each weight and bias individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we didn't zero the gradients after the update step, the algorithm would add the new gradients to the existing gradients.\n",
    "\n",
    "Separating the update step from the zero-gradient step gives extra flexibility and provides an option to not zero the gradients for e.g. gradient accumulation - where you reduce the frequency of the optimizer step and zero.grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0006, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only ran for a mini-batch because <code>xb = xtrain[:bs] and bs=64</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        \n",
    "    def __call__(self, x): return self.l2(F.relu(self.l1(x)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 10) # 10 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Pytorch docs:\n",
    "\n",
    "Returns an iterator over immediate children modules, yielding both\n",
    "the name of the module as well as the module itself.\n",
    "\n",
    "Yields:\n",
    "(string, Module): Tuple containing a name and child module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, l in model.named_children(): print(f'{name}: {l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs): # loop through epochs\n",
    "        for i in range((n-1)//bs + 1): \n",
    "            start_i = i * bs # 0 64\n",
    "            end_i = start_i + bs # 64 128\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            loss = loss_func(model(xb), yb) # calculate loss on batch\n",
    "        \n",
    "            loss.backward() # calculate gradients\n",
    "            with torch.no_grad(): # allows for in-place changing of parameters\n",
    "                for p in model.parameters(): p -= p.grad * lr  \n",
    "                model.zero_grad()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0006, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "super().\\__init__ implements the functionality of the module below when we create a class (I think?)\n",
    "\n",
    "on yield https://pythontips.com/2013/09/29/the-python-yield-keyword-explained/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModule():\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        \n",
    "    def __setattr__(self, k, v):\n",
    "        if not k.startswith(\"_\"): self._modules[k] = v # _modules gives it a name and turns it into a Pytorch nn.Module\n",
    "        super().__setattr__(k,v) # make fn call itself on init\n",
    "        \n",
    "    def __repr__(self): return f'{self._modules}'\n",
    "    \n",
    "    def parameters(self):\n",
    "        for l in self._modules.values():\n",
    "            # yield speeds up computation because we dont want to save a copy to a list before it gets used \n",
    "            for p in l.paramters(): yield p      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l) # give it a name and add as Pythorch module\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.ModuleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch implementation\n",
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequentialModel(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the equivalent to <code>nn.Sequential</code> from Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5): \n",
    "        self.params, self.lr=list(params), lr\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * lr\n",
    "    \n",
    "    # note how there is a data.zero vs. the model.zero_grad() in the fit fn above\n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not using model.zero_grad() so that user can choose which gradients to optimize for e.g. gradual unfreezing or differential learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can do:\n",
    "for epoch in range(epochs): # loop through epochs\n",
    "        for i in range((n-1)//bs + 1): \n",
    "            start_i = i * bs # 0 64\n",
    "            end_i = start_i + bs # 64 128\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            loss = loss_func(model(xb), yb) # calculate loss on batch\n",
    "        \n",
    "            loss.backward() # calculate gradients\n",
    "            opt.step()\n",
    "            opt.zero_grad()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2708, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to initialize model with SGD optimizer\n",
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2772, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt = get_model() # init model\n",
    "loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can do:\n",
    "for epoch in range(epochs): # loop through epochs\n",
    "        for i in range((n-1)//bs + 1): \n",
    "            start_i = i * bs # 0 64\n",
    "            end_i = start_i + bs # 64 128\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            loss = loss_func(model(xb), yb) # calculate loss on batch\n",
    "        \n",
    "            loss.backward() # calculate gradients\n",
    "            opt.step()\n",
    "            opt.zero_grad()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0883, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random test are usefull - so that these models can hint at problems\n",
    "assert acc>0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tests are written without a rixed random seed to check allow for model variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code is clunky when looping through mini-batches like this\n",
    "<code>xb = x_train[start_i:end_i]\n",
    "yb = y_train[start_i:end_i]</code>\n",
    "\n",
    "# something like this instead would be more helpful\n",
    "<code>xb, yb = train_ds[i*bs: i*bs+bs] </code> start_i :(up to) start_i + batch_size for every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x, self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "assert len(train_ds) == len(x_train)\n",
    "assert len(valid_ds) == len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = train_ds[0:5]\n",
    "assert xb.shape == (5, 28*28)\n",
    "assert yb.shape == (5,)\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs): # loop through epochs\n",
    "        for i in range((n-1)//bs + 1): \n",
    "            xb, yb = train_ds[i*bs : i*bs+bs]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb) # calculate loss on batch\n",
    "        \n",
    "            loss.backward() # calculate gradients\n",
    "            opt.step()\n",
    "            opt.zero_grad()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0742, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader\n",
    "\n",
    "We want to change \n",
    "\n",
    "<code>for i in range((n-1)//bs + 1): xb, yb = train_ds[i\\*bs : i\\*bs+bs]</code>\n",
    "\n",
    "to something more like\n",
    "\n",
    "<code>for xb, yb in train_dl: ...</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): self.ds, self.bs = ds, bs\n",
    "    def __iter__(self):\n",
    "        # range fn moves from from 0 to the length of dataset with batch-size length steps in between\n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yield is a co-routine used for situations when the same item needs to be called many times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>iter</code> generates the co-routine for yield\n",
    "\n",
    "<code>next</code> grabs the next item from the yield co-routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "assert xb.shape == (bs, 28*28)\n",
    "assert yb.shape == (bs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ 1h4 of FastAI course 2019 Lesson 9\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0749, grad_fn=<NllLossBackward>), tensor(0.9844))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc > 0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a problem because we are always looping thorugh the training set in order\n",
    "\n",
    "We'd like the training set to be in a random order, which should differ at each iteration. But the validation set shouldn't be randomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        # only the length of the dataset is stored\n",
    "        self.n, self.bs, self.shuffle = len(ds), bs, shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = Dataset(*train_ds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-8bc574b64da1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmall_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "small_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sampler(small_ds, 3, False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([3, 8, 6]), tensor([9, 0, 1]), tensor([4, 5, 2]), tensor([7])]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sampler(small_ds, 3, True)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs, ys = zip(*b)\n",
    "    # stacks tensor list into single list - so that a single \"Dataload\" can be returned\n",
    "    return torch.stack(xs),torch.stack(ys)\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds, self.sampler, self.collate_fn = ds, sampler, collate_fn\n",
    "    # the iter function is calling a co-routine on a co-routine    \n",
    "    def __iter__(self):\n",
    "        # grabs all indexes in sample and creates a list and stacks them into a single list of tensors \n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
    "valid_samp = Sampler(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, train_samp)\n",
    "valid_dl = DataLoader(train_ds, valid_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAN9klEQVR4nO3df4xV9ZnH8c+zWP6QojBrOhKKSyEGg8ZON4gbl6w1hvojGhw1TSexoZE4/YNJaLIhNewf1WwwZBU2SzTNTKMWNl1qEzUgaQouoOzGhDgiKo5LdQ2mTEaowZEf/mCHefaPezBTnfu9w7nn3nOZ5/1Kbu6957nnnicnfDi/7pmvubsATH5/VXYDAJqDsANBEHYgCMIOBEHYgSAuaubCzIxT/0CDubuNN72uLbuZ3Wpmh8zsPTN7sJ7vAtBYlvc6u5lNkfRHSUslHZH0qqQudx9IzMOWHWiwRmzZF0t6z93fd/czkn4raVkd3weggeoJ+2xJfxrz/kg27S+YWbeZ9ZtZfx3LAlCnhp+gc/c+SX0Su/FAmerZsg9KmjPm/bezaQBaUD1hf1XSlWb2HTObKulHkrYV0xaAouXejXf3ETPrkbRD0hRJT7n724V1BqBQuS+95VoYx+xAwzXkRzUALhyEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBJF7yGZcGKZMmZKsX3rppQ1dfk9PT9XaxRdfnJx3wYIFyfrKlSuT9ccee6xqraurKznv559/nqyvW7cuWX/44YeT9TLUFXYzOyzppKSzkkbcfVERTQEoXhFb9pvc/aMCvgdAA3HMDgRRb9hd0k4ze83Musf7gJl1m1m/mfXXuSwAdah3N36Juw+a2bckvWhm/+Pue8d+wN37JPVJkpl5ncsDkFNdW3Z3H8yej0l6XtLiIpoCULzcYTezaWY2/dxrST+QdLCoxgAUq57d+HZJz5vZue/5D3f/QyFdTTJXXHFFsj516tRk/YYbbkjWlyxZUrU2Y8aM5Lz33HNPsl6mI0eOJOsbN25M1js7O6vWTp48mZz3jTfeSNZffvnlZL0V5Q67u78v6bsF9gKggbj0BgRB2IEgCDsQBGEHgiDsQBDm3rwftU3WX9B1dHQk67t3707WG32baasaHR1N1u+///5k/dSpU7mXPTQ0lKx//PHHyfqhQ4dyL7vR3N3Gm86WHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeC4Dp7Adra2pL1ffv2Jevz5s0rsp1C1ep9eHg4Wb/pppuq1s6cOZOcN+rvD+rFdXYgOMIOBEHYgSAIOxAEYQeCIOxAEIQdCIIhmwtw/PjxZH316tXJ+h133JGsv/7668l6rT+pnHLgwIFkfenSpcn66dOnk/Wrr766am3VqlXJeVEstuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EAT3s7eASy65JFmvNbxwb29v1dqKFSuS8953333J+pYtW5J1tJ7c97Ob2VNmdszMDo6Z1mZmL5rZu9nzzCKbBVC8iezG/1rSrV+Z9qCkXe5+paRd2XsALaxm2N19r6Sv/h50maRN2etNku4quC8ABcv72/h2dz83WNaHktqrfdDMuiV151wOgILUfSOMu3vqxJu790nqkzhBB5Qp76W3o2Y2S5Ky52PFtQSgEfKGfZuk5dnr5ZK2FtMOgEapuRtvZlskfV/SZWZ2RNIvJK2T9DszWyHpA0k/bGSTk92JEyfqmv+TTz7JPe8DDzyQrD/zzDPJeq0x1tE6aobd3buqlG4uuBcADcTPZYEgCDsQBGEHgiDsQBCEHQiCW1wngWnTplWtvfDCC8l5b7zxxmT9tttuS9Z37tyZrKP5GLIZCI6wA0EQdiAIwg4EQdiBIAg7EARhB4LgOvskN3/+/GR9//79yfrw8HCyvmfPnmS9v7+/au2JJ55IztvMf5uTCdfZgeAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIrrMH19nZmaw//fTTyfr06dNzL3vNmjXJ+ubNm5P1oaGhZD0qrrMDwRF2IAjCDgRB2IEgCDsQBGEHgiDsQBBcZ0fSNddck6xv2LAhWb/55vyD/fb29ibra9euTdYHBwdzL/tClvs6u5k9ZWbHzOzgmGkPmdmgmR3IHrcX2SyA4k1kN/7Xkm4dZ/q/untH9vh9sW0BKFrNsLv7XknHm9ALgAaq5wRdj5m9me3mz6z2ITPrNrN+M6v+x8gANFzesP9S0nxJHZKGJK2v9kF373P3Re6+KOeyABQgV9jd/ai7n3X3UUm/krS42LYAFC1X2M1s1pi3nZIOVvssgNZQ8zq7mW2R9H1Jl0k6KukX2fsOSS7psKSfunvNm4u5zj75zJgxI1m/8847q9Zq3StvNu7l4i/t3r07WV+6dGmyPllVu85+0QRm7Bpn8pN1dwSgqfi5LBAEYQeCIOxAEIQdCIKwA0FwiytK88UXXyTrF12Uvlg0MjKSrN9yyy1Vay+99FJy3gsZf0oaCI6wA0EQdiAIwg4EQdiBIAg7EARhB4KoedcbYrv22muT9XvvvTdZv+6666rWal1Hr2VgYCBZ37t3b13fP9mwZQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBILjOPsktWLAgWe/p6UnW77777mT98ssvP++eJurs2bPJ+tBQ+q+Xj46OFtnOBY8tOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwXX2C0Cta9ldXeMNtFtR6zr63Llz87RUiP7+/mR97dq1yfq2bduKbGfSq7llN7M5ZrbHzAbM7G0zW5VNbzOzF83s3ex5ZuPbBZDXRHbjRyT9o7svlPR3klaa2UJJD0ra5e5XStqVvQfQomqG3d2H3H1/9vqkpHckzZa0TNKm7GObJN3VqCYB1O+8jtnNbK6k70naJ6nd3c/9OPlDSe1V5umW1J2/RQBFmPDZeDP7pqRnJf3M3U+MrXlldMhxB2109z53X+Tui+rqFEBdJhR2M/uGKkH/jbs/l00+amazsvosScca0yKAItTcjTczk/SkpHfcfcOY0jZJyyWty563NqTDSaC9fdwjnC8tXLgwWX/88ceT9auuuuq8eyrKvn37kvVHH320am3r1vQ/GW5RLdZEjtn/XtKPJb1lZgeyaWtUCfnvzGyFpA8k/bAxLQIoQs2wu/t/Sxp3cHdJNxfbDoBG4eeyQBCEHQiCsANBEHYgCMIOBMEtrhPU1tZWtdbb25uct6OjI1mfN29erp6K8MorryTr69evT9Z37NiRrH/22Wfn3RMagy07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgQR5jr79ddfn6yvXr06WV+8eHHV2uzZs3P1VJRPP/20am3jxo3JeR955JFk/fTp07l6Quthyw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQYS5zt7Z2VlXvR4DAwPJ+vbt25P1kZGRZD11z/nw8HByXsTBlh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgjB3T3/AbI6kzZLaJbmkPnf/NzN7SNIDkv6cfXSNu/++xnelFwagbu4+7qjLEwn7LEmz3H2/mU2X9Jqku1QZj/2Uuz820SYIO9B41cI+kfHZhyQNZa9Pmtk7ksr90ywAztt5HbOb2VxJ35O0L5vUY2ZvmtlTZjazyjzdZtZvZv11dQqgLjV347/8oNk3Jb0saa27P2dm7ZI+UuU4/p9V2dW/v8Z3sBsPNFjuY3ZJMrNvSNouaYe7bxinPlfSdne/psb3EHagwaqFveZuvJmZpCclvTM26NmJu3M6JR2st0kAjTORs/FLJP2XpLckjWaT10jqktShym78YUk/zU7mpb6LLTvQYHXtxheFsAONl3s3HsDkQNiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQii2UM2fyTpgzHvL8umtaJW7a1V+5LoLa8ie/ubaoWm3s/+tYWb9bv7otIaSGjV3lq1L4ne8mpWb+zGA0EQdiCIssPeV/LyU1q1t1btS6K3vJrSW6nH7ACap+wtO4AmIexAEKWE3cxuNbNDZvaemT1YRg/VmNlhM3vLzA6UPT5dNobeMTM7OGZam5m9aGbvZs/jjrFXUm8Pmdlgtu4OmNntJfU2x8z2mNmAmb1tZquy6aWuu0RfTVlvTT9mN7Mpkv4oaamkI5JeldTl7gNNbaQKMzssaZG7l/4DDDP7B0mnJG0+N7SWmf2LpOPuvi77j3Kmu/+8RXp7SOc5jHeDeqs2zPhPVOK6K3L48zzK2LIvlvSeu7/v7mck/VbSshL6aHnuvlfS8a9MXiZpU/Z6kyr/WJquSm8twd2H3H1/9vqkpHPDjJe67hJ9NUUZYZ8t6U9j3h9Ra4337pJ2mtlrZtZddjPjaB8zzNaHktrLbGYcNYfxbqavDDPeMusuz/Dn9eIE3dctcfe/lXSbpJXZ7mpL8soxWCtdO/2lpPmqjAE4JGl9mc1kw4w/K+ln7n5ibK3MdTdOX01Zb2WEfVDSnDHvv51NawnuPpg9H5P0vCqHHa3k6LkRdLPnYyX38yV3P+ruZ919VNKvVOK6y4YZf1bSb9z9uWxy6etuvL6atd7KCPurkq40s++Y2VRJP5K0rYQ+vsbMpmUnTmRm0yT9QK03FPU2Scuz18slbS2xl7/QKsN4VxtmXCWvu9KHP3f3pj8k3a7KGfn/lfRPZfRQpa95kt7IHm+X3ZukLars1v2fKuc2Vkj6a0m7JL0r6T8ltbVQb/+uytDeb6oSrFkl9bZElV30NyUdyB63l73uEn01Zb3xc1kgCE7QAUEQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ/w8ie3GmjcGk5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's returning random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7151, grad_fn=<NllLossBackward>), tensor(0.7812))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "fit()\n",
    "\n",
    "loss, acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc > 0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from torch.utils.data  import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @1h9mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0417, grad_fn=<NllLossBackward>), tensor(0.9844))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0369, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.opt = get_model()\n",
    "fit()\n",
    "loss, acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc > 0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't implement <code>num_workers</code> which calls multiple threads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "You should <strong>always</strong> have a validation set as well, in order to identify overfitting\n",
    "\n",
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "We need to call <code>model.train()</code> before training and <code>model.eval()</code> before inference to insure the right behavior for <code>nn.BatchNorm2d</code> and <code>nn.Dropout</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # Handle batchnorm /dropout\n",
    "        model.train()\n",
    "        \n",
    "        for xb, yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        # change to eval mode - turn off batchnorm and dropout during validation set loop\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # init loss, accuracy as floats\n",
    "            tot_loss, tot_acc = 0.,0.\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_func(pred, yb)\n",
    "                tot_acc  += accuracy(pred, yb)\n",
    "            nv = len(valid_dl)\n",
    "            print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this will not work very well if batch-size varies, because we'd actually need to be using the weighted averages of the mini-batches, where the weight will depend on the size of the mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            # we can double the batch size because we're not doing a backward pass for validation, so no need to store gradients\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.1956) tensor(0.9422)\n",
      "1 tensor(0.1223) tensor(0.9632)\n",
      "2 tensor(0.1100) tensor(0.9670)\n",
      "3 tensor(0.1601) tensor(0.9526)\n",
      "4 tensor(0.0965) tensor(0.9720)\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "train_dl, valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "loss, acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 03_minibatch_training.ipynb to nb_03.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 03_minibatch_training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Training loop\n",
    "\n",
    "0. setup training data\n",
    "\n",
    "1. create model\n",
    "2. calculate loss\n",
    "3. calculate gradients\n",
    "\n",
    "Tensorboard works\n",
    "Check S Gugger. fast progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@1h14min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks\n",
    "\n",
    "Allow for full customization of each step in the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
